import streamlit as st
import pandas as pd
import io
import requests
import json

# === ç½‘é¡µé…ç½® ===
st.set_page_config(page_title="äºšé©¬é€Šå¹¿å‘Šå…¨èƒ½ç‹ (ä¿®å¤ç‰ˆ)", layout="wide", page_icon="ğŸ›¡ï¸")
st.title("ğŸ›¡ï¸ Amazon å¹¿å‘Šä¼˜åŒ– (ç¨³å¦‚æ³°å±±ç‰ˆ)")
st.info("ğŸ’¡ å·²ç§»é™¤å¤–éƒ¨ç»˜å›¾ä¾èµ–ï¼Œä¿®å¤äº† 'matplotlib' æŠ¥é”™é—®é¢˜ï¼Œä¿è¯æµç•…è¿è¡Œï¼")

# === ä¾§è¾¹æ  ===
st.sidebar.header("ğŸ”‘ AI è®¾ç½®")
default_key = ""
deepseek_key = st.sidebar.text_input("DeepSeek API Key", value=default_key, type="password")
product_name = st.sidebar.text_input("äº§å“åç§°", value="LED Makeup Mirror")

st.sidebar.header("âš™ï¸ ç«ä»·è§„åˆ™")
target_acos = st.sidebar.slider("ğŸ¯ ç›®æ ‡ ACoS", 10, 60, 30) / 100

st.sidebar.markdown("---")
st.sidebar.info("ğŸ“‰ **å¦è¯ç­–ç•¥**ï¼š\nåˆ—å‡ºæ‰€æœ‰ 0 è®¢å•è¯ï¼ŒæŒ‰èŠ±è´¹ä»é«˜åˆ°ä½æ’åºã€‚")

# === ä¸Šä¼ åŒºåŸŸ ===
st.write("---")
c1, c2 = st.columns(2)
with c1:
    file_bulk = st.file_uploader("ğŸ“‚ 1. æ‹–å…¥ã€æ‰¹é‡æ“ä½œè¡¨æ ¼ã€‘(Bulk)", type=['xlsx'], key="bulk")
with c2:
    file_term = st.file_uploader("ğŸ“‚ 2. æ‹–å…¥ã€æœç´¢è¯æŠ¥å‘Šã€‘(Search Term)", type=['xlsx'], key="term")
st.write("---")

# === DeepSeek å‡½æ•° ===
def call_deepseek_analysis(api_key, product, neg_data, bid_data):
    url = "https://api.deepseek.com/chat/completions"
    prompt = f"""
    æˆ‘æ˜¯äºšé©¬é€Šå–å®¶ï¼Œäº§å“æ˜¯ã€{product}ã€‘ã€‚
    
    1. ã€æµªè´¹èµ„é‡‘æ’è¡Œæ¦œ (0è½¬åŒ–)ã€‘ï¼š
    {neg_data.to_string(index=False)}
    * é‡ç‚¹åˆ†æå‰3ä¸ªèŠ±è´¹æœ€é«˜çš„è¯ï¼Œä¸ºä»€ä¹ˆä¸å‡ºå•ï¼Ÿï¼ˆè¯ä¹‰å¤ªå®½ï¼Ÿç«å“å¤ªå¼ºï¼Ÿï¼‰
    * æ˜¯å¦å»ºè®®ç«‹å³å¦å®šï¼Ÿ

    2. ã€ç«ä»·ä¼˜åŒ–å»ºè®®ã€‘ï¼š
    {bid_data.to_string(index=False)}
    
    è¯·ç”¨ Markdown æ ¼å¼ï¼Œç»™å‡ºç›´æ¥çš„æ“ä½œå»ºè®®ã€‚
    """
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.5
    }
    try:
        res = requests.post(url, headers=headers, json=data)
        if res.status_code == 200: return res.json()['choices'][0]['message']['content']
        return f"AI æŠ¥é”™: {res.text}"
    except Exception as e: return f"é”™è¯¯: {e}"

# ==========================================
# 1ï¸âƒ£ ç«ä»·ä¼˜åŒ– (Bulk File)
# ==========================================
ai_bid_summary = pd.DataFrame()

if file_bulk:
    try:
        dfs = pd.read_excel(file_bulk, sheet_name=None, engine='openpyxl')
        bulk_df = pd.DataFrame()

        for name, df in dfs.items():
            df.columns = df.columns.astype(str).str.strip()
            cols = df.columns.tolist()
            if ('èŠ±è´¹' in cols or 'Spend' in cols) and ('å…³é”®è¯æ–‡æœ¬' in cols or 'Keyword Text' in cols):
                bulk_df = df
                break
        
        if bulk_df.empty:
            st.error("âŒ Bulk æ–‡ä»¶æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ•°æ®ï¼")
        else:
            col_map = {
                'å®ä½“å±‚çº§': 'Record Type', 'Record Type': 'Record Type',
                'å¹¿å‘Šæ´»åŠ¨åç§°ï¼ˆä»…ä¾›å‚è€ƒï¼‰': 'Campaign', 'å¹¿å‘Šæ´»åŠ¨åç§°': 'Campaign',
                'å¹¿å‘Šç»„åç§°ï¼ˆä»…ä¾›å‚è€ƒï¼‰': 'Ad Group', 'å¹¿å‘Šç»„åç§°': 'Ad Group',
                'åŒ¹é…ç±»å‹': 'Match Type', 'Match Type': 'Match Type',
                'å…³é”®è¯æ–‡æœ¬': 'Keyword', 'Keyword Text': 'Keyword',
                'ç«ä»·': 'Max Bid', 'Max Bid': 'Max Bid',
                'èŠ±è´¹': 'Spend', 'Spend': 'Spend',
                'é”€é‡': 'Sales', 'Sales': 'Sales',
                'è®¢å•æ•°é‡': 'Orders', 'Orders': 'Orders',
                'ç‚¹å‡»é‡': 'Clicks', 'Clicks': 'Clicks',
                'æ‹“å±•å•†å“æŠ•æ”¾ç¼–å·': 'Targeting', 'å•†å“æŠ•æ”¾ ID': 'Targeting ID'
            }
            df_clean = bulk_df.rename(columns=col_map)
            df_clean = df_clean.loc[:, ~df_clean.columns.duplicated()]

            if 'Keyword' not in df_clean.columns: df_clean['Keyword'] = None
            df_clean['Target'] = df_clean['Keyword']
            if 'Targeting' in df_clean.columns: 
                df_clean['Target'] = df_clean['Target'].fillna(df_clean['Targeting'])

            for c in ['Spend', 'Sales', 'Orders', 'Max Bid']:
                if c in df_clean.columns: df_clean[c] = pd.to_numeric(df_clean[c], errors='coerce').fillna(0)

            df_clean['ACoS'] = df_clean['Spend'] / df_clean['Sales']
            df_clean['ACoS'] = df_clean['ACoS'].fillna(0)

            bad_bids = df_clean[(df_clean['Orders'] > 0) & (df_clean['ACoS'] > target_acos)].copy()
            bad_bids = bad_bids.sort_values(by='ACoS', ascending=False)

            st.subheader("1ï¸âƒ£ ç«ä»·ä¼˜åŒ–å»ºè®® (ACoS è¶…æ ‡æ¦œ)")
            if not bad_bids.empty:
                show_cols = ['Campaign', 'Ad Group', 'Target', 'Match Type', 'Max Bid', 'Orders', 'Spend', 'ACoS']
                final_cols = [c for c in show_cols if c in bad_bids.columns]
                ai_bid_summary = bad_bids[final_cols].head(5)
                
                # ç®€å•é«˜äº® ACoS
                st.dataframe(bad_bids[final_cols].style.format({'ACoS': '{:.1%}', 'Spend': '{:.2f}', 'Max Bid': '{:.2f}'}), use_container_width=True)
            else:
                st.success("âœ… ç«ä»·æ§åˆ¶è‰¯å¥½ã€‚")

    except Exception as e:
        st.error(f"Bulk è¯»å–é”™è¯¯: {e}")

# ==========================================
# 2ï¸âƒ£ å¦è¯æ’å (Search Term Report)
# ==========================================
ai_waste_data = pd.DataFrame()

if file_term:
    try:
        term_df = pd.read_excel(file_term, engine='openpyxl')
        term_df.columns = term_df.columns.astype(str).str.strip()

        # æš´åŠ›åŒ¹é…è®¢å•åˆ—
        order_col = None
        for col in term_df.columns:
            if "è®¢å•" in col or "Orders" in col:
                order_col = col
                break
        if order_col: term_df.rename(columns={order_col: 'Orders'}, inplace=True)

        st_col_map = {
            'å®¢æˆ·æœç´¢è¯': 'Search Term', 'Customer Search Term': 'Search Term',
            'å¹¿å‘Šæ´»åŠ¨åç§°': 'Campaign', 'Campaign Name': 'Campaign',
            'å¹¿å‘Šç»„åç§°': 'Ad Group', 'Ad Group Name': 'Ad Group',
            'èŠ±è´¹': 'Spend', 'Spend': 'Spend',
            'ç‚¹å‡»é‡': 'Clicks', 'Clicks': 'Clicks'
        }
        term_df = term_df.rename(columns=st_col_map)
        term_df = term_df.loc[:, ~term_df.columns.duplicated()]

        for c in ['Spend', 'Orders', 'Clicks']:
             if c in term_df.columns: term_df[c] = pd.to_numeric(term_df[c], errors='coerce').fillna(0)

        if 'Orders' in term_df.columns:
            # æ ¸å¿ƒé€»è¾‘ï¼š0å•ï¼ŒæŒ‰èŠ±è´¹æ’åº
            zero_order_df = term_df[term_df['Orders'] == 0].copy()
            zero_order_df = zero_order_df.sort_values(by='Spend', ascending=False)
            
            top_waste = zero_order_df.head(20)
            ai_waste_data = top_waste.head(10)

            st.subheader("2ï¸âƒ£ æµªè´¹èµ„é‡‘æ’è¡Œæ¦œ (Top 20 0å•è¯)")
            if not top_waste.empty:
                st.error(f"ğŸš¨ å‘ç° {len(top_waste)} ä¸ªèŠ±è´¹æœ€é«˜çš„ä¸å‡ºå•è¯ï¼")
                
                show_cols = ['Campaign', 'Ad Group', 'Search Term', 'Spend', 'Clicks']
                final_cols = [c for c in show_cols if c in top_waste.columns]
                
                # âš ï¸ å»æ‰äº† background_gradientï¼Œæ”¹ç”¨ç®€å•çš„è¡¨æ ¼å±•ç¤º
                st.dataframe(
                    top_waste[final_cols].style.format({'Spend': '{:.2f}'}), 
                    use_container_width=True
                )
            else:
                st.info("æ²¡æœ‰ 0 å•çš„è¯ã€‚")
        else:
            st.error(f"âŒ æ‰¾ä¸åˆ°'è®¢å•'åˆ—ã€‚åˆ—åï¼š{list(term_df.columns)}")

    except Exception as e:
        st.error(f"æœç´¢è¯æŠ¥å‘Šè¯»å–é”™è¯¯: {e}")

# === 3. AI ç»¼åˆæ±‡æŠ¥ ===
if file_bulk and file_term:
    st.write("---")
    st.subheader("ğŸ¤– DeepSeek æ¯’èˆŒè¯Šæ–­")
    if st.button("å¼€å§‹ AI åˆ†æ"):
        if not deepseek_key:
             st.error("Key ä¸ºç©ºï¼")
        else:
            with st.spinner("AI åˆ†æä¸­..."):
                report = call_deepseek_analysis(deepseek_key, product_name, ai_waste_data, ai_bid_summary)
                st.markdown(report)

