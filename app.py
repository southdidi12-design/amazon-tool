import streamlit as st
import pandas as pd
import requests
import json
import os
from datetime import datetime

# === 1. å…¨å±€é…ç½® ===
st.set_page_config(
    page_title="Amazon AI è®­ç»ƒå¸ˆ (v5.2 è¯Šæ–­ç‰ˆ)", 
    layout="wide", 
    page_icon="ğŸ©º",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main { background-color: #f8f9fa; }
    div[data-testid="stMetric"] { background-color: white; border: 1px solid #ddd; padding: 10px; border-radius: 8px; }
    .stTabs [data-baseweb="tab-list"] { gap: 8px; }
    .stButton>button { width: 100%; border-radius: 4px; }
    .stAlert { padding: 10px; border-radius: 5px; }
</style>
""", unsafe_allow_html=True)

# === 2. æ ¸å¿ƒé€»è¾‘ ===
DATA_FILE = "deepseek_cot_data.jsonl"

def generate_and_save_ai_thought(api_key, term, spend, clicks, orders, user_intent):
    if not api_key:
        st.error("âŒ éœ€è¦ API Key")
        return None
    prompt = f"æˆ‘æ˜¯äºšé©¬é€Šè¿è¥ã€‚äº§å“Makeup Mirrorã€‚åˆ†æè¯'{term}'ï¼ŒèŠ±è´¹${spend}ï¼Œç‚¹å‡»{clicks}ï¼Œè®¢å•{orders}ã€‚è¯·è¾“å‡ºJSONï¼š1. reasoning(åˆ†æè¿‡ç¨‹) 2. action(å»ºè®®æ“ä½œ)ã€‚æˆ‘çš„å€¾å‘æ˜¯ï¼š{user_intent}ã€‚"
    try:
        with st.spinner(f"ğŸ§  AI åˆ†æä¸­..."):
            res = requests.post(
                "https://api.deepseek.com/chat/completions",
                headers={"Authorization": f"Bearer {api_key}"},
                json={"model": "deepseek-chat", "messages": [{"role": "user", "content": prompt}], "temperature": 0.7, "response_format": {"type": "json_object"}}
            )
            if res.status_code == 200:
                ai_json = json.loads(res.json()['choices'][0]['message']['content'])
                data = {"messages": [{"role": "system", "content": "PPCä¸“å®¶"}, {"role": "user", "content": f"è¯:{term},è´¹:{spend}"}, {"role": "assistant", "content": f"é€»è¾‘:{ai_json.get('reasoning')}\nå»ºè®®:{ai_json.get('action')}"}]}
                with open(DATA_FILE, "a", encoding="utf-8") as f: f.write(json.dumps(data, ensure_ascii=False) + "\n")
                st.toast("âœ… å·²ä¿å­˜")
                return ai_json.get('reasoning')
    except Exception as e: st.error(f"Error: {e}")

# === 3. ä¾§è¾¹æ  ===
st.sidebar.title("ğŸ©º æ§åˆ¶å° v5.2")
default_key = "sk-55cc3f56742f4e43be099c9489e02911"
deepseek_key = st.sidebar.text_input("ğŸ”‘ DeepSeek Key", value=default_key, type="password")
product_name = st.sidebar.text_input("ğŸ“¦ äº§å“åç§°", value="Makeup Mirror")

if os.path.exists(DATA_FILE):
    with open(DATA_FILE, "r", encoding="utf-8") as f: count = sum(1 for _ in f)
    st.sidebar.metric("ğŸ“š å·²ç§¯ç´¯æ•™æ", f"{count} æ¡")
    with open(DATA_FILE, "r", encoding="utf-8") as f: st.sidebar.download_button("ğŸ“¥ ä¸‹è½½æ•°æ®", f, file_name="finetune.jsonl")

# === 4. ä¸»ç•Œé¢ ===
st.title("ğŸ©º Amazon AI è®­ç»ƒå¸ˆ (v5.2 è¯Šæ–­ä¿®å¤ç‰ˆ)")

c1, c2 = st.columns(2)
with c1:
    file_bulk = st.file_uploader("ğŸ“‚ 1. ä¸Šä¼  Bulk è¡¨æ ¼ (å¿…é¡»å« Record Type)", type=['xlsx', 'csv'], key="bulk")
with c2:
    file_term = st.file_uploader("ğŸ“‚ 2. ä¸Šä¼  Search Term (ç”¨äºè®­ç»ƒ)", type=['xlsx', 'csv'], key="term")

def load_data(file):
    if not file: return pd.DataFrame()
    try:
        if file.name.endswith('.csv'): return pd.read_csv(file)
        else: return pd.read_excel(file, engine='openpyxl') # ç®€åŒ–è¯»å–é€»è¾‘ï¼Œå…ˆè¯»è¿›æ¥å†è¯´
    except Exception as e:
        st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return pd.DataFrame()

df_bulk = load_data(file_bulk)
df_term = load_data(file_term)

# æ¸…æ´—åˆ—å
if not df_bulk.empty: df_bulk.columns = df_bulk.columns.astype(str).str.strip()
if not df_term.empty: df_term.columns = df_term.columns.astype(str).str.strip()

# === è¯Šæ–­é€»è¾‘ (æ–°å¢) ===
bk_cols = {}
if not df_bulk.empty:
    # å°è¯•å¯»æ‰¾å…³é”®åˆ—
    bk_cols = {
        'entity': next((c for c in df_bulk.columns if c in ["å®ä½“å±‚çº§", "Record Type", "Entity"]), None),
        'kw': next((c for c in df_bulk.columns if c in ["å…³é”®è¯æ–‡æœ¬", "Keyword Text", "Keyword"]), None),
        'bid': next((c for c in df_bulk.columns if c in ["ç«ä»·", "Keyword Bid", "Bid"]), None),
        'spend': next((c for c in df_bulk.columns if c in ["èŠ±è´¹", "Spend"]), None),
        'sales': next((c for c in df_bulk.columns if c in ["é”€é‡", "Sales"]), None),
        'orders': next((c for c in df_bulk.columns if c in ["è®¢å•æ•°é‡", "Orders"]), None),
        'clicks': next((c for c in df_bulk.columns if c in ["ç‚¹å‡»é‡", "Clicks"]), None),
    }
    
    # æ ¸å¿ƒå¤„ç†ï¼šå¦‚æœæ‰¾åˆ°äº† Entity åˆ—ï¼Œæ‰ç”Ÿæˆ df_kws
    if bk_cols['entity'] and bk_cols['kw']:
        try:
            df_kws = df_bulk[df_bulk[bk_cols['entity']].astype(str).str.contains('Keyword|å…³é”®è¯', case=False, na=False)].copy()
            for c in [bk_cols['spend'], bk_cols['sales'], bk_cols['orders'], bk_cols['clicks'], bk_cols['bid']]:
                if c: df_kws[c] = pd.to_numeric(df_kws[c], errors='coerce').fillna(0)
            if bk_cols['spend'] and bk_cols['sales']:
                df_kws['ACoS'] = df_kws.apply(lambda x: x[bk_cols['spend']]/x[bk_cols['sales']] if x[bk_cols['sales']]>0 else 0, axis=1)
        except Exception as e:
            st.error(f"æ•°æ®å¤„ç†é”™è¯¯: {e}")

# === 5. åŠŸèƒ½åŒº ===
tab1, tab2, tab3 = st.tabs(["ğŸ§  AI è‡ªåŠ¨æ ‡æ³¨", "ğŸ“ˆ çœ‹æ¿ (è¯Šæ–­ä¸­...)", "ğŸ’° ç«ä»·"])

with tab1:
    st.subheader("ğŸ§  è®­ç»ƒæ•°æ®ç§¯ç´¯")
    if not df_term.empty:
        # Search Term å¤„ç†é€»è¾‘
        st_term_col = next((c for c in df_term.columns if c in ["å®¢æˆ·æœç´¢è¯", "Search Term", "Customer Search Term"]), None)
        st_spend_col = next((c for c in df_term.columns if c in ["èŠ±è´¹", "Spend"]), None)
        
        if st_term_col and st_spend_col:
            st.success(f"âœ… Search Term è¡¨æ ¼è¯»å–æˆåŠŸï¼åŒ…å« {len(df_term)} è¡Œã€‚")
            # ... (ç®€åŒ–çš„æŒ‰é’®æ˜¾ç¤ºä»£ç ï¼Œä¿æŒä¹‹å‰é€»è¾‘) ...
            mask = (pd.to_numeric(df_term[st_spend_col], errors='coerce') > 0)
            review_df = df_term[mask].head(5)
            for idx, row in review_df.iterrows():
                if st.button(f"åˆ†æ: {row[st_term_col]}", key=f"btn_{idx}"):
                    generate_and_save_ai_thought(deepseek_key, row[st_term_col], row[st_spend_col], 0, 0, "Check")
        else:
            st.warning(f"âš ï¸ Search Term è¡¨æ ¼ç¼ºå°‘å…³é”®åˆ—ã€‚å½“å‰åˆ—åï¼š{list(df_term.columns)}")
    else:
        st.info("è¯·ä¸Šä¼  Search Term è¡¨æ ¼")

with tab2:
    st.subheader("ğŸ“ˆ è´¦æˆ·é€è§† (è¯Šæ–­æ¨¡å¼)")
    
    if not df_bulk.empty:
        if 'df_kws' in locals() and not df_kws.empty:
            # æ­£å¸¸æ˜¾ç¤ºå›¾è¡¨
            st.success("âœ… Bulk è¡¨æ ¼è§£æå®Œç¾ï¼")
            st.scatter_chart(df_kws[df_kws[bk_cols['spend']]>0], x=bk_cols['spend'], y=bk_cols['sales'], size=bk_cols['clicks'], color='ACoS')
        else:
            # ğŸš¨ è¯Šæ–­æŠ¥é”™åŒº
            st.error("âŒ è¡¨æ ¼å·²ä¸Šä¼ ï¼Œä½†æ— æ³•ç”Ÿæˆå›¾è¡¨ã€‚åŸå› å¦‚ä¸‹ï¼š")
            
            # æ£€æŸ¥1: æ˜¯å¦ç¼ºå°‘å…³é”®åˆ—ï¼Ÿ
            missing_cols = []
            if not bk_cols.get('entity'): missing_cols.append("Record Type (å®ä½“å±‚çº§)")
            if not bk_cols.get('kw'): missing_cols.append("Keyword Text (å…³é”®è¯æ–‡æœ¬)")
            
            if missing_cols:
                st.warning(f"âš ï¸ ä½ çš„è¡¨æ ¼é‡Œç¼ºå°‘è¿™äº›åˆ—åï¼š{missing_cols}")
                st.write("ğŸ‘‰ **å½“å‰è¡¨æ ¼é‡Œçš„åˆ—åæœ‰ï¼š**")
                st.code(list(df_bulk.columns))
                st.info("ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥ä½ æ˜¯ä¸æ˜¯ä¸Šä¼ äº† Search Term æŠ¥è¡¨ï¼Ÿå›¾è¡¨åŠŸèƒ½å¿…é¡»ç”¨ **Bulk Operation File (æ‰¹é‡æ“ä½œè¡¨æ ¼)**ã€‚")
            else:
                st.warning("âš ï¸ åˆ—åéƒ½å¯¹ï¼Œä½†ç­›é€‰ 'Keyword' è¡Œæ—¶ä¸ºç©ºã€‚è¯·æ£€æŸ¥ 'Record Type' åˆ—çš„å†…å®¹ã€‚")
                st.write("å‰5è¡Œæ•°æ®é¢„è§ˆï¼š")
                st.dataframe(df_bulk.head())
    else:
        st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼  Bulk è¡¨æ ¼ã€‚")

with tab3:
    st.write("ç«ä»·åŠŸèƒ½åŒº")