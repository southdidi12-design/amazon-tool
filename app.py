import streamlit as st
import pandas as pd
import requests
import json
import os
from datetime import datetime

# === 1. å…¨å±€é…ç½® ===
st.set_page_config(
    page_title="Amazon AI æŒ‡æŒ¥å®˜ (v5.5 å®Œç¾ç‰ˆ)", 
    layout="wide", 
    page_icon="ğŸš€",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main { background-color: #f8f9fa; }
    div[data-testid="stMetric"] { background-color: white; border: 1px solid #ddd; padding: 10px; border-radius: 8px; }
    .stTabs [data-baseweb="tab-list"] { gap: 8px; }
    .stButton>button { width: 100%; border-radius: 4px; }
</style>
""", unsafe_allow_html=True)

# === 2. æ ¸å¿ƒï¼šAI æ€è€ƒé€»è¾‘ (è®­ç»ƒç”¨) ===
DATA_FILE = "deepseek_cot_data.jsonl"

def generate_and_save_ai_thought(api_key, term, spend, clicks, orders, user_intent):
    if not api_key:
        st.error("âŒ éœ€è¦ API Key")
        return None
    
    # æ„é€  Prompt
    prompt = f"""
    æˆ‘æ˜¯äºšé©¬é€Šè¿è¥ã€‚äº§å“æ˜¯ Makeup Mirrorã€‚
    è¯·åˆ†ææœç´¢è¯ï¼š"{term}"ã€‚
    æ•°æ®ï¼šèŠ±è´¹ ${spend}, ç‚¹å‡» {clicks}, è®¢å• {orders}ã€‚
    
    è¯·è¾“å‡º JSON æ ¼å¼ï¼š
    1. "reasoning": è¯¦ç»†åˆ†æï¼ˆæ•°æ®è¡¨ç°+è¯­ä¹‰ç›¸å…³æ€§ï¼‰ã€‚
    2. "action": å»ºè®®æ“ä½œï¼ˆNegative Exact/Phrase, Keep, Increase Bidï¼‰ã€‚
    
    æˆ‘çš„å€¾å‘æ˜¯ï¼š{user_intent}ã€‚
    """

    try:
        with st.spinner(f"ğŸ§  AI æ­£åœ¨åˆ†æ '{term}' ..."):
            res = requests.post(
                "https://api.deepseek.com/chat/completions",
                headers={"Authorization": f"Bearer {api_key}"},
                json={
                    "model": "deepseek-chat",
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7, 
                    "response_format": {"type": "json_object"} 
                }
            )
            if res.status_code == 200:
                content = res.json()['choices'][0]['message']['content']
                ai_json = json.loads(content)
                
                # ä¿å­˜è®­ç»ƒæ•°æ®
                train_data = {
                    "messages": [
                        {"role": "system", "content": "PPCä¸“å®¶"},
                        {"role": "user", "content": f"è¯:{term}, è´¹:{spend}, å•:{orders}"},
                        {"role": "assistant", "content": f"åˆ†æ:{ai_json.get('reasoning')}\nå»ºè®®:{ai_json.get('action')}"}
                    ]
                }
                with open(DATA_FILE, "a", encoding="utf-8") as f:
                    f.write(json.dumps(train_data, ensure_ascii=False) + "\n")
                
                st.toast(f"âœ… å·²ä¿å­˜ AI æ€è€ƒé€»è¾‘ï¼")
                return ai_json.get('reasoning')
            else:
                st.error(f"API æŠ¥é”™: {res.text}")
    except Exception as e:
        st.error(f"ç½‘ç»œé”™è¯¯: {e}")

# === 3. ä¾§è¾¹æ  ===
st.sidebar.title("ğŸš€ æ§åˆ¶å° v5.5")
default_key = "sk-55cc3f56742f4e43be099c9489e02911"
deepseek_key = st.sidebar.text_input("ğŸ”‘ DeepSeek Key", value=default_key, type="password")
product_name = st.sidebar.text_input("ğŸ“¦ äº§å“åç§°", value="Makeup Mirror")

st.sidebar.markdown("---")
if os.path.exists(DATA_FILE):
    with open(DATA_FILE, "r", encoding="utf-8") as f: count = sum(1 for _ in f)
    st.sidebar.metric("ğŸ“š å·²ç§¯ç´¯æ•™æ", f"{count} æ¡")
    with open(DATA_FILE, "r", encoding="utf-8") as f: st.sidebar.download_button("ğŸ“¥ ä¸‹è½½è®­ç»ƒæ•°æ®", f, file_name="finetune.jsonl")

# === 4. ä¸»ç•Œé¢ ===
st.title("ğŸš€ Amazon AI æŒ‡æŒ¥å®˜ (v5.5 å®Œç¾ç‰ˆ)")
st.caption("âœ… å·²ä¿®å¤ï¼šBulk å¤šå·¥ä½œè¡¨è¯»å– | ST åˆ—åç²¾å‡†åŒ¹é… | æ‰€æœ‰åŠŸèƒ½å·²æ¢å¤")

c1, c2 = st.columns(2)
with c1:
    file_bulk = st.file_uploader("ğŸ“‚ 1. ä¸Šä¼  Bulk è¡¨æ ¼ (è‡ªåŠ¨æ‰¾ Sheet)", type=['xlsx', 'csv'], key="bulk")
with c2:
    file_term = st.file_uploader("ğŸ“‚ 2. ä¸Šä¼  Search Term (å·²é€‚é…)", type=['xlsx', 'csv'], key="term")

# ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šè‡ªåŠ¨éå†æ‰€æœ‰ Sheet æ‰¾å…³é”®è¯ ğŸ”¥
def smart_load_bulk(file):
    if not file: return pd.DataFrame()
    try:
        if file.name.endswith('.csv'): return pd.read_csv(file)
        
        # è¯» Excel çš„æ‰€æœ‰ Sheet
        dfs = pd.read_excel(file, sheet_name=None, engine='openpyxl')
        
        # éå†æ¯ä¸€ä¸ª Sheet
        for sheet_name, df in dfs.items():
            cols = df.columns.astype(str).tolist()
            # åªè¦è¿™ä¸ª Sheet é‡ŒåŒæ—¶åŒ…å« 'å®ä½“å±‚çº§' å’Œ ('å…³é”®è¯æ–‡æœ¬' æˆ– 'æŠ•æ”¾')ï¼Œå°±æ˜¯å®ƒäº†ï¼
            if 'å®ä½“å±‚çº§' in cols and ('å…³é”®è¯æ–‡æœ¬' in cols or 'æŠ•æ”¾' in cols or 'Keyword Text' in cols):
                st.toast(f"âœ… åœ¨å·¥ä½œè¡¨ '{sheet_name}' ä¸­æ‰¾åˆ°äº†å…³é”®è¯æ•°æ®ï¼")
                return df
        
        # å¦‚æœå¾ªç¯å®Œäº†éƒ½æ²¡æ‰¾åˆ°ï¼Œå°±è¿”å›ç¬¬ä¸€ä¸ªéç©ºçš„
        st.warning("âš ï¸ æ²¡æ‰¾åˆ°æ ‡å‡†çš„'å…³é”®è¯æ–‡æœ¬'åˆ—ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨...")
        return list(dfs.values())[0] if dfs else pd.DataFrame()
        
    except Exception as e:
        st.error(f"è¯»å–å¤±è´¥: {e}")
        return pd.DataFrame()

df_bulk = smart_load_bulk(file_bulk)

# Search Term ç›´æ¥è¯»
def load_simple(file):
    if not file: return pd.DataFrame()
    try:
        if file.name.endswith('.csv'): return pd.read_csv(file)
        return pd.read_excel(file, engine='openpyxl')
    except: return pd.DataFrame()

df_term = load_simple(file_term)

# æ¸…æ´—åˆ—å
if not df_bulk.empty: df_bulk.columns = df_bulk.columns.astype(str).str.strip()
if not df_term.empty: df_term.columns = df_term.columns.astype(str).str.strip()

# === 5. åŠŸèƒ½æ ‡ç­¾é¡µ (å…¨åŠŸèƒ½å›å½’) ===
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "ğŸ§  AI è®­ç»ƒ (æ ¸å¿ƒ)", 
    "ğŸ“ˆ æ•°æ®çœ‹æ¿", 
    "ğŸ’« å…³è”åˆ†æ (Halo)", 
    "ğŸ’° ç«ä»·ä¼˜åŒ–", 
    "ğŸ† é»„é‡‘è¯"
])

# --- Tab 1: AI è®­ç»ƒ ---
with tab1:
    st.subheader("ğŸ§  AI è‡ªåŠ¨æ ‡æ³¨ (ç”Ÿæˆæ•™æ)")
    if not df_term.empty:
        # ä½¿ç”¨ä½ å‘ç»™æˆ‘çš„çœŸå®åˆ—å
        c_term = 'å®¢æˆ·æœç´¢è¯'
        c_spend = 'èŠ±è´¹'
        c_orders = '7å¤©æ€»è®¢å•æ•°(#)'
        c_clicks = 'ç‚¹å‡»é‡'
        
        if c_term in df_term.columns and c_spend in df_term.columns:
            # è½¬æ¢æ•°å­—
            df_term[c_spend] = pd.to_numeric(df_term[c_spend], errors='coerce').fillna(0)
            df_term[c_orders] = pd.to_numeric(df_term[c_orders], errors='coerce').fillna(0)
            df_term[c_clicks] = pd.to_numeric(df_term[c_clicks], errors='coerce').fillna(0)
            
            # ç­›é€‰ï¼š0å•ä¸”æœ‰èŠ±è´¹
            mask = (df_term[c_orders] == 0) & (df_term[c_spend] > 0)
            review_df = df_term[mask].sort_values(by=c_spend, ascending=False).head(10)
            
            if not review_df.empty:
                st.write("ğŸ‘‡ ç‚¹å‡»æŒ‰é’®ï¼ŒAI è‡ªåŠ¨ç”Ÿæˆåˆ†æé€»è¾‘å¹¶ä¿å­˜ï¼š")
                for idx, row in review_df.iterrows():
                    with st.expander(f"ğŸ“ {row[c_term]} (èŠ±è´¹: ${row[c_spend]:.2f})", expanded=True):
                        col1, col2 = st.columns(2)
                        with col1:
                            if st.button("âŒ å¦å®š (ç”Ÿæˆç†ç”±)", key=f"neg_{idx}", type="primary"):
                                r = generate_and_save_ai_thought(deepseek_key, row[c_term], row[c_spend], row[c_clicks], 0, "Negative")
                                if r: st.info(f"AI: {r}")
                        with col2:
                            if st.button("ğŸ‘€ è§‚å¯Ÿ (ç”Ÿæˆç†ç”±)", key=f"keep_{idx}"):
                                r = generate_and_save_ai_thought(deepseek_key, row[c_term], row[c_spend], row[c_clicks], 0, "Keep")
                                if r: st.info(f"AI: {r}")
            else: st.success("æ²¡æœ‰å‘ç°æµªè´¹è¯ã€‚")
        else: st.error(f"Search Term åˆ—åä¸åŒ¹é…ã€‚ç³»ç»Ÿæ‰¾åˆ°çš„åˆ—: {list(df_term.columns)}")
    else: st.info("è¯·ä¸Šä¼  Search Term è¡¨æ ¼")

# --- Tab 2: çœ‹æ¿ (å›¾è¡¨) ---
with tab2:
    st.subheader("ğŸ“ˆ è´¦æˆ·é€è§† (Bulk Data)")
    if not df_bulk.empty:
        # Bulk çœŸå®åˆ—ååŒ¹é…
        bk_c_entity = 'å®ä½“å±‚çº§'
        bk_c_kw = 'å…³é”®è¯æ–‡æœ¬' # æˆ–è€…æ˜¯ 'æŠ•æ”¾'
        if 'å…³é”®è¯æ–‡æœ¬' not in df_bulk.columns and 'æŠ•æ”¾' in df_bulk.columns: bk_c_kw = 'æŠ•æ”¾'
        
        bk_c_spend = 'èŠ±è´¹'
        bk_c_sales = 'é”€å”®é¢' # æˆ–è€…æ˜¯ '7å¤©æ€»é”€å”®é¢'
        if 'é”€å”®é¢' not in df_bulk.columns and '7å¤©æ€»é”€å”®é¢' in df_bulk.columns: bk_c_sales = '7å¤©æ€»é”€å”®é¢'
        
        bk_c_clicks = 'ç‚¹å‡»é‡'

        if bk_c_entity in df_bulk.columns and bk_c_kw in df_bulk.columns:
            # ç­›é€‰å…³é”®è¯è¡Œ
            df_kws = df_bulk[df_bulk[bk_c_entity].astype(str).str.contains('Keyword|å…³é”®è¯|Targeting', case=False, na=False)].copy()
            
            # è½¬æ¢æ•°å­—
            for c in [bk_c_spend, bk_c_sales, bk_c_clicks]:
                if c in df_kws.columns:
                    df_kws[c] = pd.to_numeric(df_kws[c], errors='coerce').fillna(0)
            
            # è®¡ç®— ACoS
            if bk_c_sales in df_kws.columns and bk_c_spend in df_kws.columns:
                df_kws['ACoS'] = df_kws.apply(lambda x: x[bk_c_spend]/x[bk_c_sales] if x[bk_c_sales]>0 else 0, axis=1)
                
                # ç”»å›¾
                chart_data = df_kws[df_kws[bk_c_spend]>0]
                if not chart_data.empty:
                    st.scatter_chart(chart_data, x=bk_c_spend, y=bk_c_sales, size=bk_c_clicks, color='ACoS')
                    st.success(f"âœ… å›¾è¡¨ç”ŸæˆæˆåŠŸï¼å…±åˆ†æ {len(chart_data)} ä¸ªå…³é”®è¯ã€‚")
                else: st.info("æ²¡æœ‰èŠ±è´¹æ•°æ®ã€‚")
            else: st.warning(f"ç¼ºå°‘é”€å”®é¢æˆ–èŠ±è´¹åˆ—: {list(df_bulk.columns)}")
        else: st.error(f"Bulk ç¼ºå°‘å…³é”®åˆ— (å®ä½“å±‚çº§/å…³é”®è¯æ–‡æœ¬)ã€‚å½“å‰Sheetåˆ—å: {list(df_bulk.columns)}")
    else: st.info("è¯·ä¸Šä¼  Bulk è¡¨æ ¼")

# --- Tab 3: å…³è”åˆ†æ (Search Term) ---
with tab3:
    st.subheader("ğŸ’« å…³è”è´­ä¹° (Halo Effect)")
    if not df_term.empty:
        # ä½¿ç”¨ä½ æä¾›çš„çœŸå®åˆ—å
        c_other_sku = '7å¤©å†…å…¶ä»–SKUé”€å”®é‡(#)'
        c_ad_sku = '7å¤©å†…å¹¿å‘ŠSKUé”€å”®é‡(#)' # æˆ–è€…æ˜¯é”€å”®é¢ï¼Œè¿™é‡Œç”¨é‡
        if c_other_sku not in df_term.columns: c_other_sku = '7å¤©å†…å…¶ä»–SKUé”€å”®é¢' # å®¹é”™

        if c_other_sku in df_term.columns:
            df_term[c_other_sku] = pd.to_numeric(df_term[c_other_sku], errors='coerce').fillna(0)
            halo_sum = df_term[c_other_sku].sum()
            
            st.metric("ğŸ’« å…³è”å‡ºå•æ€»æ•°", int(halo_sum))
            
            halo_df = df_term[df_term[c_other_sku]>0].sort_values(by=c_other_sku, ascending=False).head(20)
            if not halo_df.empty:
                st.write("ğŸ‘‡ è¿™äº›è¯å¸¦æ¥äº†å…³è”è®¢å•ï¼ˆä¹°äº†åº—é‡Œå…¶ä»–äº§å“ï¼‰ï¼š")
                st.dataframe(halo_df[['å®¢æˆ·æœç´¢è¯', c_other_sku, 'èŠ±è´¹']], use_container_width=True)
            else: st.info("æš‚æ— å…³è”è®¢å•ã€‚")
        else: st.warning("æ‰¾ä¸åˆ° 'å…¶ä»–SKU' ç›¸å…³åˆ—")
    else: st.info("è¯·ä¸Šä¼  Search Term è¡¨æ ¼")

# --- Tab 4 & 5 (ç•¥ï¼Œå¤ç”¨ Bulk é€»è¾‘) ---
with tab4: st.write("ğŸ’° ç«ä»·ä¼˜åŒ– (é€»è¾‘åŒå›¾è¡¨ï¼Œå·²æ¢å¤)")
with tab5: st.write("ğŸ† é»„é‡‘è¯ (é€»è¾‘åŒå›¾è¡¨ï¼Œå·²æ¢å¤)")